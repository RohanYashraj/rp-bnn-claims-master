
@article{kuo2019deeptriangle,
  title={{DeepTriangle}: A deep learning approach to loss reserving},
  author={Kuo, Kevin},
  journal={Risks},
  volume={7},
  number={3},
  pages={97},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{blundellWeightUncertainty2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.05424},
  primaryClass = {cs, stat},
  title = {Weight {{Uncertainty}} in {{Neural Networks}}},
  abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
  journal = {arXiv:1505.05424 [cs, stat]},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  month = may,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{boumezouedIndividualClaims2017,
  title = {Individual Claims Reserving: A Survey},
  shorttitle = {Individual Claims Reserving},
  abstract = {This paper surveys the stochastic modelling of individual claims occurrence and development for reserving purposes in non-life (general) insurance. The paper revisits the continuous time stochastic modelling framework of Norberg (1993) and Hesselager (1994), and provides a consistent presentation of the modelling, inference, and forecasting (with simulation and closed-forms) of individual claims histories as well as aggregate quantities as the overall reserve for both RBNS and IBNR claims. Numerical illustrations are given based on real portfolio datasets, as well as comparisons with classical triangle-based methods.},
  author = {Boumezoued, Alexandre and Devineau, Laurent},
  month = nov,
  year = {2017},
  keywords = {loss-reserving,Individual claims reserving,Non-life/General insurance,Poisson point processes,Process and estimation errors,Stochastic reserving,Thinning algorithm}
}

@article{wuthrichMachineLearning2018,
  title = {Machine Learning in Individual Claims Reserving},
  volume = {2018},
  number = {6},
  journal = {Scandinavian Actuarial Journal},
  author = {W{\"u}thrich, Mario V.},
  year = {2018},
  pages = {465--480}
}

@article{gabrielliNeuralNetwork2019a,
  title = {Neural Network Embedding of the Over-Dispersed {{Poisson}} Reserving Model},
  journal = {Scandinavian Actuarial Journal},
  author = {Gabrielli, Andrea and Richman, Ronald and W{\"u}thrich, Mario V.},
  year = {2019},
  pages = {1--29}
}

@article{wuthrichNeuralNetworks2018,
  title = {Neural Networks Applied to Chain\textendash{}Ladder Reserving},
  volume = {8},
  number = {2},
  journal = {European Actuarial Journal},
  author = {W{\"u}thrich, Mario V.},
  year = {2018},
  pages = {407--436}
}

@article{gabrielliIndividualClaims2018,
  title = {An Individual Claims History Simulation Machine},
  volume = {6},
  number = {2},
  journal = {Risks},
  author = {Gabrielli, Andrea and V W{\"u}thrich, Mario},
  year = {2018},
  pages = {29}
}

@book{nealBayesianLearning2012,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  isbn = {978-1-4612-0745-0},
  abstract = {Artificial "neural networks" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the "overfitting" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence.},
  language = {en},
  publisher = {{Springer Science \& Business Media}},
  author = {Neal, Radford M.},
  month = dec,
  year = {2012},
  keywords = {Computers / Computer Simulation,Computers / Desktop Applications / Design \& Graphics,Computers / Intelligence (AI) \& Semantics,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{taylorLossReserving2019,
  title = {Loss {{Reserving Models}}: {{Granular}} and {{Machine Learning Forms}}},
  volume = {7},
  shorttitle = {Loss {{Reserving Models}}},
  number = {3},
  journal = {Risks},
  author = {Taylor, Greg},
  year = {2019},
  pages = {82}
}

@article{bishopMixtureDensity1994,
  title = {Mixture {{Density Networks}}},
  author = {Bishop, Christopher M.},
  year = {1994}
}

@article{duvalIndividualLoss2019,
  title = {Individual {{Loss Reserving Using}} a {{Gradient Boosting}}-{{Based Approach}}},
  volume = {7},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  abstract = {In this paper, we propose models for non-life loss reserving combining traditional approaches such as Mack\&rsquo;s or generalized linear models and gradient boosting algorithm in an individual framework. These claim-level models use information about each of the payments made for each of the claims in the portfolio, as well as characteristics of the insured. We provide an example based on a detailed dataset from a property and casualty insurance company. We contrast some traditional aggregate techniques, at the portfolio-level, with our individual-level approach and we discuss some points related to practical applications.},
  language = {en},
  number = {3},
  journal = {Risks},
  doi = {10.3390/risks7030079},
  author = {Duval, Francis and Pigeon, Mathieu},
  month = sep,
  year = {2019},
  keywords = {gradient boosting,individual models,loss reserving,predictive modeling},
  pages = {79}
}

@article{lopezTreeBasedAlgorithm2019,
  title = {A {{Tree}}-{{Based Algorithm Adapted}} to {{Microlevel Reserving}} and {{Long Development Claims}}},
  journal = {ASTIN Bulletin: The Journal of the IAA},
  author = {Lopez, Olivier and Milhaud, Xavier and Th{\'e}rond, Pierre-E.},
  year = {2019},
  pages = {1--22}
}

@article{baudryMachineLearning,
  title = {A Machine Learning Approach for Individual Claims Reserving in Insurance},
  journal = {Applied Stochastic Models in Business and Industry},
  author = {Baudry, Maximilien and Robert, Christian Y.}
}

@article{antonioMicrolevelStochastic2014,
  title = {Micro-Level Stochastic Loss Reserving for General Insurance},
  volume = {2014},
  number = {7},
  journal = {Scandinavian Actuarial Journal},
  author = {Antonio, Katrien and Plat, Richard},
  year = {2014},
  pages = {649--669}
}

@article{pigeonIndividualLoss2013,
  title = {Individual Loss Reserving with the Multivariate Skew Normal Framework},
  volume = {43},
  number = {3},
  journal = {ASTIN Bulletin: The Journal of the IAA},
  author = {Pigeon, Mathieu and Antonio, Katrien and Denuit, Michel},
  year = {2013},
  pages = {399--428}
}

@article{pigeonIndividualLoss2014,
  title = {Individual Loss Reserving Using Paid\textendash{}Incurred Data},
  volume = {58},
  journal = {Insurance: Mathematics and Economics},
  author = {Pigeon, Mathieu and Antonio, Katrien and Denuit, Michel},
  year = {2014},
  pages = {121--131}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  publisher = {{MIT press}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016}
}

@inproceedings{gravesPracticalVariational2011,
  title = {Practical Variational Inference for Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Graves, Alex},
  year = {2011},
  pages = {2348--2356}
}

@book{rdevelopmentcoreteamLanguageEnvironment2011,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  shorttitle = {R},
  publisher = {{R foundation for statistical computing Vienna, Austria}},
  author = {R Development Core Team, RFFSC},
  year = {2011}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{guoEntityEmbeddings2016,
  title = {Entity Embeddings of Categorical Variables},
  journal = {arXiv preprint arXiv:1604.06737},
  author = {Guo, Cheng and Berkhahn, Felix},
  year = {2016}
}

@article{hochreiterLongShortterm1997,
  title = {Long Short-Term Memory},
  volume = {9},
  number = {8},
  journal = {Neural computation},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  pages = {1735--1780}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  volume = {521},
  number = {7553},
  journal = {Nature},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  pages = {436}
}

@inproceedings{kendallWhatUncertainties2017,
  title = {What Uncertainties Do We Need in {Bayesian} Deep Learning for Computer Vision?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kendall, Alex and Gal, Yarin},
  year = {2017},
  pages = {5574--5584}
}

@article{wuthrichNonlifeInsurance2017,
  title = {Non-Life Insurance: Mathematics \& Statistics},
  shorttitle = {Non-Life Insurance},
  journal = {Available at SSRN 2319328},
  author = {Wuthrich, Mario V.},
  year = {2017}
}

@techreport{gabrielliNeuralNetwork2019,
  address = {{Rochester, NY}},
  type = {{{SSRN Scholarly Paper}}},
  title = {A {{Neural Network Boosted Double Over}}-{{Dispersed Poisson Claims Reserving Model}}},
  abstract = {We present an actuarial loss reserving technique that takes into account both claim counts and claim amounts. Separate (over-dispersed) Poisson models for the claim counts and the claim amounts are combined by a joint embedding into a neural network architecture. As starting point of the neural network calibration we use exactly these two separate (over-dispersed) Poisson models. Such a nested model can be interpreted as a boosting machine. It allows us for joint modeling and mutual learning of claim counts and claim amounts beyond the two individual (over-dispersed) Poisson models. Moreover, this choice of neural network initialization guarantees stability and accelerates representation learning.},
  language = {en},
  number = {ID 3365517},
  institution = {{Social Science Research Network}},
  author = {Gabrielli, Andrea},
  month = apr,
  year = {2019},
  keywords = {boosting,chain-ladder reserves,claim amounts,claim counts,claims reserving in insurance,cross-classified over-dispersed Poisson model,double chain-ladder,embedding,learning across portfolios,neural network}
}


